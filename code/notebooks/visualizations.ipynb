{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import torchvision\n",
    "import glob\n",
    "from PIL import Image\n",
    "import gc\n",
    "import pickle\n",
    "import os\n",
    "import math\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprint_output(out, n_max_synsets=10):\n",
    "    wids = []\n",
    "    best_ids = out.argsort()[::-1][:n_max_synsets]\n",
    "    for u in best_ids:\n",
    "        wids.append(str(synsets[corr_inv[u] - 1][1][0]))\n",
    "    return wids\n",
    "\n",
    "def top5accuracy(true, predicted):\n",
    "    \"\"\"\n",
    "    Function to predict the top 5 accuracy\n",
    "    \"\"\"\n",
    "    assert len(true) == len(predicted)\n",
    "    result = []\n",
    "    flag  = 0\n",
    "    for i in range(len(true)):\n",
    "        flag  = 0\n",
    "        temp = true[i]\n",
    "        for j in predicted[i][0:5]:\n",
    "            if j == temp:\n",
    "                flag = 1\n",
    "                break\n",
    "        if flag == 1:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "    counter = 0.\n",
    "    for i in result:\n",
    "        if i == 1:\n",
    "            counter += 1.\n",
    "    error = 1.0 - counter/float(len(result))\n",
    "    #print len(np.where(np.asarray(result) == 1)[0])\n",
    "    return len(np.where(np.asarray(result) == 1)[0]), error\n",
    "\n",
    "\n",
    "def get_ranks(true, predicted):\n",
    "    assert len(true) == len(predicted)\n",
    "    ranks = []\n",
    "    for i,row in enumerate(predicted):\n",
    "        ranks.append((np.asarray(row)==true[i]).nonzero()[0].item())\n",
    "    return ranks\n",
    "\n",
    "def mean_rank_deficit(original_ranks, predicted_ranks):\n",
    "    \"\"\"\n",
    "    Average number of ranks the correct class dropped in predicted ranks compared with\n",
    "    the true or original ranks. If ranks improved, we can't say anything about the lesion\n",
    "    so we just return zero which means that no deficit occured.\n",
    "    \"\"\"\n",
    "    assert len(original_ranks) == len(predicted_ranks)\n",
    "    diff = np.asarray(predicted_ranks,dtype=float)-np.asarray(original_ranks,dtype=float)\n",
    "    diff[diff < 0] = 0 # If the model improves we count that as no deficit\n",
    "    return np.mean(diff)\n",
    "\n",
    "class HookedModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Constructs a model for applying forward hooks.\n",
    "    Interface:\n",
    "        1. Choose model\n",
    "        2. Choose layer\n",
    "        3. Choose mask\n",
    "        4. Apply mask\n",
    "        5. Evaluate model\n",
    "        6. Remove mask\n",
    "    \"\"\"\n",
    "    def __init__(self,model):\n",
    "        super(HookedModel,self).__init__()\n",
    "        self.model = model\n",
    "        self.hooks = []\n",
    "    \n",
    "    def forward(self,*args,**kwargs):\n",
    "        return self.model(*args,**kwargs)\n",
    "\n",
    "    def apply_hook(self,layer_name,hook):\n",
    "        self.hooks.append(\n",
    "            get_module(self.model,layer_name).register_forward_hook(hook)\n",
    "        )\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        for _ in range(len(self.hooks)):\n",
    "            hook = self.hooks.pop()\n",
    "            hook.remove()\n",
    "\n",
    "\n",
    "class OutputMaskHook:\n",
    "    \"\"\"\n",
    "    Hook for applying elementwise mask to output of layer.\n",
    "    \"\"\"\n",
    "    def __init__(self,mask):\n",
    "        self.mask = mask\n",
    "\n",
    "    def __call__(self, model, input, output):\n",
    "        output = torch.mul(output, self.mask) # Elementwise multiplication\n",
    "        return output\n",
    "\n",
    "def get_module(model, name):\n",
    "    \"\"\"\n",
    "    Finds the named module within the given model.\n",
    "    Courtesy of https://github.com/kmeng01/rome/blob/main/util/nethook.py#L355\n",
    "    \"\"\"\n",
    "    for n, m in model.named_modules():\n",
    "        if n == name:\n",
    "            return m\n",
    "    raise LookupError(name)\n",
    "\n",
    "def activation_to_magnitude(coordinates):\n",
    "    \"\"\"\n",
    "    Magnitude is proportional to the sum of animate and inanimate activations\n",
    "    \n",
    "    dot([x,y],[1,1]) / norm([1,1])\n",
    "    \"\"\"\n",
    "    magnitude = np.sum(coordinates,axis=1)/np.sqrt(2)\n",
    "    return magnitude \n",
    "\n",
    "\n",
    "def activation_to_selectivity(coordinates):\n",
    "    \"\"\"\n",
    "    Selectivity is proportional to Animate - Inanimate activations\n",
    "    \n",
    "    dot([x,y],[-1,1]) / norm([-1,1])\n",
    "    \"\"\"\n",
    "    selectivity = (coordinates[:,0] - coordinates[:,1])/np.sqrt(2)\n",
    "    return selectivity\n",
    "\n",
    "\n",
    "def grid_space(x,y,y_partitions=28,x_partitions=28,symmetric=False):\n",
    "    \"\"\"\n",
    "    Takes in set of coordinates in 2D space and returns geopandas.GeoDataFrame\n",
    "    where each entry represents a single cell in the grid space with an equal number\n",
    "    of units. Symmetric grids don't necessarily have the same number of units per cell.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        x (1D list) : List of x coordinates\n",
    "        y (1D list) : List of y coordinates\n",
    "        y_partitions: Number of partitions in Y direction, should be even number so that\n",
    "                      cells can be symmetrical around zero\n",
    "        x_partitions: Number of partitions in X direction, should be even number so that\n",
    "                      cells can be symmetrical around zero\n",
    "        symmetric (bool): Whether or not to make it symmetric around zero\n",
    "                      \n",
    "    Returns\n",
    "    -------\n",
    "        geopandas.GeoDataFrame : one entry per cell in grid starting at bottom left and going right\n",
    "        \n",
    "    \"\"\"\n",
    "    if symmetric:\n",
    "        y_neg_sorted = np.sort(y[y<0])\n",
    "        y_pos_sorted = np.sort(y[y>0])\n",
    "\n",
    "        # First half of bounds come from negative region, second from positive\n",
    "        y_bounds = ([y_neg_sorted[int(((2*i)/y_partitions)*len(y_neg_sorted))] \n",
    "                         for i in range(int(y_partitions/2))] + [0] +\n",
    "                    [y_pos_sorted[int(((2*i)/y_partitions)*len(y_pos_sorted))] \n",
    "                         for i in range(1,int(y_partitions/2))] + [y_pos_sorted[-1]])\n",
    "    else:\n",
    "        y_sorted = np.sort(y)\n",
    "        y_bounds = ([y_sorted[math.floor((i/y_partitions)*len(y_sorted))] \n",
    "                         for i in range(y_partitions)] + [y_sorted[-1]])\n",
    "\n",
    "    grid_cells = []\n",
    "    \n",
    "    for i,y_lower_bound in enumerate(y_bounds[:-1]):\n",
    "        y_upper_bound = y_bounds[i+1]\n",
    "        \n",
    "        if symmetric:\n",
    "            # Only look at x coordinates which fall within vertical (y direction) strip of interest\n",
    "            x_neg_sorted = np.sort(x[(y > y_lower_bound) & (y < y_upper_bound) & (x < 0)])\n",
    "            x_pos_sorted = np.sort(x[(y > y_lower_bound) & (y < y_upper_bound) & (x > 0)])\n",
    "\n",
    "            # First half of bounds come from negative region, second from positive\n",
    "            x_bounds = ([x_neg_sorted[int(((2*k)/x_partitions)*len(x_neg_sorted))] \n",
    "                             for k in range(int(x_partitions/2))] + [0] +\n",
    "                        [x_pos_sorted[int(((2*k)/x_partitions)*len(x_pos_sorted))] \n",
    "                             for k in range(1,int(x_partitions/2))] + [x_pos_sorted[-1]])\n",
    "        else:\n",
    "            x_sorted = np.sort(x[(y > y_lower_bound) & (y < y_upper_bound)])\n",
    "            x_bounds = ([x_sorted[int((k/x_partitions)*len(x_sorted))] \n",
    "                             for k in range(x_partitions)] + [x_sorted[-1]])\n",
    "        \n",
    "        # Add bounds to list\n",
    "        for j,x_lower_bound in enumerate(x_bounds[:-1]):\n",
    "            x_upper_bound = x_bounds[j+1]\n",
    "            # grid_cells.append(shapely.geometry.box(x_lower_bound, y_lower_bound, \n",
    "            #                                        x_upper_bound, y_upper_bound))\n",
    "            grid_cells.append([x_lower_bound, y_lower_bound, \n",
    "                               x_upper_bound, y_upper_bound])\n",
    "    \n",
    "    # I don't know what this CRS projection is...\n",
    "    # crs = \"+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +a=6371007.181 +b=6371007.181 +units=m +no_defs\"\n",
    "    # return geopandas.GeoDataFrame(grid_cells, columns=['geometry'], crs=crs)\n",
    "\n",
    "    return grid_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['schooner','brain_coral','junco_bird','snail','grey_whale','siberian_husky','electric_fan','bookcase','fountain_pen','toaster']\n",
    "\n",
    "\n",
    "dfs = []\n",
    "for cl in classes:\n",
    "    d = pd.read_csv('../data/experiments/iclr_results_'+cl+'_mps.csv',index_col=0)\n",
    "    dfs.append(d)\n",
    "\n",
    "    d = pd.read_csv('../data/experiments/iclr_results_'+cl+'_first_layers.csv',index_col=0)\n",
    "    dfs.append(d)\n",
    "\n",
    "    d = pd.read_csv('../data/experiments/iclr_results_'+cl+'.csv',index_col=0)\n",
    "    dfs.append(d)\n",
    "\n",
    "d = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['layer1.0', 'layer1.1', 'layer1.2', 'layer2.0', 'layer2.1',\n",
       "       'layer2.2', 'layer2.3', 'layer3.0', 'layer3.1', 'layer3.2',\n",
       "       'layer3.3', 'layer3.4', 'layer3.5', 'layer4.0', 'layer4.1',\n",
       "       'layer4.2'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['layer'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.to_csv('../data/experiments/iclr_results_master.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['layer1.0', 'layer1.1', 'layer1.2', 'layer2.0', 'layer2.1',\n",
       "       'layer2.2', 'layer2.3', 'layer3.0', 'layer3.1', 'layer3.2',\n",
       "       'layer3.3', 'layer3.4', 'layer3.5', 'layer4.0', 'layer4.1',\n",
       "       'layer4.2'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['layer'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('../data/experiments/iclr_results_master.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in ['decoding_accuracy_delta','ablation_impact']:\n",
    "    for b in ['cka', 'procrustes','pwcca', 'mean_sq_cca_corr', 'mean_cca_corr']:\n",
    "        df = pd.DataFrame([],columns=['class','layer','correlation','pvalue'])\n",
    "        for cl in d['class'].unique():\n",
    "            for layer in d['layer'].unique():\n",
    "                data = d[(d['class']==cl) & (d['layer']==layer)]\n",
    "                corr,p = spearmanr(data[a],data[b])\n",
    "                df.loc[len(df)] = [cl,layer,corr,p]\n",
    "        df.to_csv('../data/experiments/resnet50_'+a+'_X_'+b+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>layer</th>\n",
       "      <th>tile</th>\n",
       "      <th>decoding_accuracy_delta</th>\n",
       "      <th>regularized_decoding_accuracy</th>\n",
       "      <th>ablation_impact</th>\n",
       "      <th>cka</th>\n",
       "      <th>procrustes</th>\n",
       "      <th>pwcca</th>\n",
       "      <th>mean_sq_cca_corr</th>\n",
       "      <th>mean_cca_corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>schooner</td>\n",
       "      <td>encoder.layers.encoder_layer_0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.262716</td>\n",
       "      <td>0.34</td>\n",
       "      <td>139.08</td>\n",
       "      <td>0.131366</td>\n",
       "      <td>0.048820</td>\n",
       "      <td>0.335492</td>\n",
       "      <td>0.403575</td>\n",
       "      <td>0.620655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>schooner</td>\n",
       "      <td>encoder.layers.encoder_layer_0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.051592</td>\n",
       "      <td>0.26</td>\n",
       "      <td>321.26</td>\n",
       "      <td>0.129247</td>\n",
       "      <td>0.038593</td>\n",
       "      <td>0.371744</td>\n",
       "      <td>0.362221</td>\n",
       "      <td>0.588581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>schooner</td>\n",
       "      <td>encoder.layers.encoder_layer_0</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.005847</td>\n",
       "      <td>0.31</td>\n",
       "      <td>256.48</td>\n",
       "      <td>0.136942</td>\n",
       "      <td>0.042650</td>\n",
       "      <td>0.392409</td>\n",
       "      <td>0.338484</td>\n",
       "      <td>0.567730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>schooner</td>\n",
       "      <td>encoder.layers.encoder_layer_0</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.058081</td>\n",
       "      <td>0.29</td>\n",
       "      <td>354.26</td>\n",
       "      <td>0.135191</td>\n",
       "      <td>0.053938</td>\n",
       "      <td>0.359716</td>\n",
       "      <td>0.373101</td>\n",
       "      <td>0.594471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>schooner</td>\n",
       "      <td>encoder.layers.encoder_layer_0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.038859</td>\n",
       "      <td>0.04</td>\n",
       "      <td>470.82</td>\n",
       "      <td>0.117733</td>\n",
       "      <td>0.070713</td>\n",
       "      <td>0.582259</td>\n",
       "      <td>0.161389</td>\n",
       "      <td>0.378549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>schooner</td>\n",
       "      <td>encoder.layers.encoder_layer_0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.038859</td>\n",
       "      <td>0.03</td>\n",
       "      <td>586.08</td>\n",
       "      <td>0.248796</td>\n",
       "      <td>0.118507</td>\n",
       "      <td>0.685399</td>\n",
       "      <td>0.094782</td>\n",
       "      <td>0.284539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>schooner</td>\n",
       "      <td>encoder.layers.encoder_layer_0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.038859</td>\n",
       "      <td>0.05</td>\n",
       "      <td>553.40</td>\n",
       "      <td>0.250019</td>\n",
       "      <td>0.112281</td>\n",
       "      <td>0.673245</td>\n",
       "      <td>0.101402</td>\n",
       "      <td>0.296234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>schooner</td>\n",
       "      <td>encoder.layers.encoder_layer_0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.038859</td>\n",
       "      <td>0.06</td>\n",
       "      <td>375.76</td>\n",
       "      <td>0.114889</td>\n",
       "      <td>0.070571</td>\n",
       "      <td>0.582726</td>\n",
       "      <td>0.161034</td>\n",
       "      <td>0.378467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>schooner</td>\n",
       "      <td>encoder.layers.encoder_layer_0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.038859</td>\n",
       "      <td>0.06</td>\n",
       "      <td>367.00</td>\n",
       "      <td>0.109656</td>\n",
       "      <td>0.069008</td>\n",
       "      <td>0.581627</td>\n",
       "      <td>0.161978</td>\n",
       "      <td>0.379376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>schooner</td>\n",
       "      <td>encoder.layers.encoder_layer_0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.038859</td>\n",
       "      <td>0.09</td>\n",
       "      <td>578.30</td>\n",
       "      <td>0.238037</td>\n",
       "      <td>0.114413</td>\n",
       "      <td>0.681933</td>\n",
       "      <td>0.096511</td>\n",
       "      <td>0.287639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>schooner</td>\n",
       "      <td>encoder.layers.encoder_layer_0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.038859</td>\n",
       "      <td>0.13</td>\n",
       "      <td>564.18</td>\n",
       "      <td>0.259464</td>\n",
       "      <td>0.125514</td>\n",
       "      <td>0.690895</td>\n",
       "      <td>0.091941</td>\n",
       "      <td>0.279620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>schooner</td>\n",
       "      <td>encoder.layers.encoder_layer_0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.038859</td>\n",
       "      <td>0.04</td>\n",
       "      <td>401.74</td>\n",
       "      <td>0.115520</td>\n",
       "      <td>0.074220</td>\n",
       "      <td>0.602629</td>\n",
       "      <td>0.146483</td>\n",
       "      <td>0.359135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>schooner</td>\n",
       "      <td>encoder.layers.encoder_layer_0</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.058988</td>\n",
       "      <td>0.33</td>\n",
       "      <td>312.92</td>\n",
       "      <td>0.118322</td>\n",
       "      <td>0.049372</td>\n",
       "      <td>0.339278</td>\n",
       "      <td>0.398738</td>\n",
       "      <td>0.616356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>schooner</td>\n",
       "      <td>encoder.layers.encoder_layer_0</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.039898</td>\n",
       "      <td>0.30</td>\n",
       "      <td>156.72</td>\n",
       "      <td>0.086888</td>\n",
       "      <td>0.033387</td>\n",
       "      <td>0.382865</td>\n",
       "      <td>0.348201</td>\n",
       "      <td>0.575570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>schooner</td>\n",
       "      <td>encoder.layers.encoder_layer_0</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.099763</td>\n",
       "      <td>0.35</td>\n",
       "      <td>173.34</td>\n",
       "      <td>0.104964</td>\n",
       "      <td>0.034881</td>\n",
       "      <td>0.383030</td>\n",
       "      <td>0.348024</td>\n",
       "      <td>0.575735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>schooner</td>\n",
       "      <td>encoder.layers.encoder_layer_0</td>\n",
       "      <td>15</td>\n",
       "      <td>-0.272568</td>\n",
       "      <td>0.46</td>\n",
       "      <td>66.94</td>\n",
       "      <td>0.086873</td>\n",
       "      <td>0.039858</td>\n",
       "      <td>0.360599</td>\n",
       "      <td>0.372378</td>\n",
       "      <td>0.594895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       class                           layer  tile  decoding_accuracy_delta  \\\n",
       "0   schooner  encoder.layers.encoder_layer_0     0                -0.262716   \n",
       "1   schooner  encoder.layers.encoder_layer_0     1                -0.051592   \n",
       "2   schooner  encoder.layers.encoder_layer_0     2                -0.005847   \n",
       "3   schooner  encoder.layers.encoder_layer_0     3                -0.058081   \n",
       "4   schooner  encoder.layers.encoder_layer_0     4                 0.038859   \n",
       "5   schooner  encoder.layers.encoder_layer_0     5                 0.038859   \n",
       "6   schooner  encoder.layers.encoder_layer_0     6                 0.038859   \n",
       "7   schooner  encoder.layers.encoder_layer_0     7                 0.038859   \n",
       "8   schooner  encoder.layers.encoder_layer_0     8                 0.038859   \n",
       "9   schooner  encoder.layers.encoder_layer_0     9                 0.038859   \n",
       "10  schooner  encoder.layers.encoder_layer_0    10                 0.038859   \n",
       "11  schooner  encoder.layers.encoder_layer_0    11                 0.038859   \n",
       "12  schooner  encoder.layers.encoder_layer_0    12                -0.058988   \n",
       "13  schooner  encoder.layers.encoder_layer_0    13                -0.039898   \n",
       "14  schooner  encoder.layers.encoder_layer_0    14                -0.099763   \n",
       "15  schooner  encoder.layers.encoder_layer_0    15                -0.272568   \n",
       "\n",
       "    regularized_decoding_accuracy  ablation_impact       cka  procrustes  \\\n",
       "0                            0.34           139.08  0.131366    0.048820   \n",
       "1                            0.26           321.26  0.129247    0.038593   \n",
       "2                            0.31           256.48  0.136942    0.042650   \n",
       "3                            0.29           354.26  0.135191    0.053938   \n",
       "4                            0.04           470.82  0.117733    0.070713   \n",
       "5                            0.03           586.08  0.248796    0.118507   \n",
       "6                            0.05           553.40  0.250019    0.112281   \n",
       "7                            0.06           375.76  0.114889    0.070571   \n",
       "8                            0.06           367.00  0.109656    0.069008   \n",
       "9                            0.09           578.30  0.238037    0.114413   \n",
       "10                           0.13           564.18  0.259464    0.125514   \n",
       "11                           0.04           401.74  0.115520    0.074220   \n",
       "12                           0.33           312.92  0.118322    0.049372   \n",
       "13                           0.30           156.72  0.086888    0.033387   \n",
       "14                           0.35           173.34  0.104964    0.034881   \n",
       "15                           0.46            66.94  0.086873    0.039858   \n",
       "\n",
       "       pwcca  mean_sq_cca_corr  mean_cca_corr  \n",
       "0   0.335492          0.403575       0.620655  \n",
       "1   0.371744          0.362221       0.588581  \n",
       "2   0.392409          0.338484       0.567730  \n",
       "3   0.359716          0.373101       0.594471  \n",
       "4   0.582259          0.161389       0.378549  \n",
       "5   0.685399          0.094782       0.284539  \n",
       "6   0.673245          0.101402       0.296234  \n",
       "7   0.582726          0.161034       0.378467  \n",
       "8   0.581627          0.161978       0.379376  \n",
       "9   0.681933          0.096511       0.287639  \n",
       "10  0.690895          0.091941       0.279620  \n",
       "11  0.602629          0.146483       0.359135  \n",
       "12  0.339278          0.398738       0.616356  \n",
       "13  0.382865          0.348201       0.575570  \n",
       "14  0.383030          0.348024       0.575735  \n",
       "15  0.360599          0.372378       0.594895  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[(d['class']==classes[0]) & (d['layer']==layers[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tile\n",
       "0     174.365333\n",
       "1     272.654333\n",
       "2     296.253833\n",
       "3     272.641333\n",
       "4     289.421500\n",
       "5     321.612000\n",
       "6     322.600333\n",
       "7     288.383833\n",
       "8     299.625833\n",
       "9     322.738833\n",
       "10    319.875833\n",
       "11    284.168000\n",
       "12    271.885833\n",
       "13    299.348167\n",
       "14    271.383500\n",
       "15    163.010833\n",
       "Name: ablation_impact, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.groupby('tile')['ablation_impact'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(d['ablation_impact'],bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/'\n",
    "\n",
    "# Load model\n",
    "model = torchvision.models.vit_b_16(weights='DEFAULT').to('cuda:0')\n",
    "layers = [n for n,m in model.named_modules() if isinstance(m,torchvision.models.vision_transformer.EncoderBlock)]\n",
    "\n",
    "#Load the details of all the 1000 classes and the function to convert the synset id to words\n",
    "meta_clsloc_file = data_path+'meta_clsloc.mat'\n",
    "synsets = loadmat(meta_clsloc_file)['synsets'][0]\n",
    "synsets_imagenet_sorted = sorted([(int(s[0]), str(s[1][0])) for s in synsets[:1000]],key=lambda v: v[1])\n",
    "corr = {}\n",
    "for j in range(1000):\n",
    "    corr[synsets_imagenet_sorted[j][0]] = j\n",
    "\n",
    "corr_inv = {}\n",
    "for j in range(1, 1001):\n",
    "    corr_inv[corr[j]] = j\n",
    "\n",
    "#Code snippet to load the ground truth labels to measure the performance\n",
    "truth = {}\n",
    "with open(data_path+'ILSVRC2014_clsloc_validation_ground_truth.txt') as f:\n",
    "    line_num = 1\n",
    "    for line in f.readlines():\n",
    "        ind_ = int(line)\n",
    "        temp  = None\n",
    "        for i in synsets_imagenet_sorted:\n",
    "            if i[0] == ind_:\n",
    "                temp = i\n",
    "        #print ind_,temp\n",
    "        if temp != None:\n",
    "            truth[line_num] = temp\n",
    "        else:\n",
    "            print('##########', ind_)\n",
    "            pass\n",
    "        line_num += 1\n",
    "\n",
    "# Get list of all images\n",
    "im_valid_test = glob.glob(data_path+'images/*')\n",
    "im_valid_test = np.asarray(im_valid_test)\n",
    "\n",
    "# Make list of wids\n",
    "true_valid_wids = []\n",
    "for i in im_valid_test:\n",
    "    temp1 = i.split('/')[-1]\n",
    "    temp = temp1.split('.')[0].split('_')[-1]\n",
    "    true_valid_wids.append(truth[int(temp)][1])\n",
    "true_valid_wids = np.asarray(true_valid_wids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snag-lab/anaconda3/envs/torch/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500,)\n",
      "baseline 486 500 0.028000000000000025 0.972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = []\n",
    "for im in im_valid_test:\n",
    "    im_temp = Image.open(im).convert('RGB')\n",
    "    preprocess = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1.transforms(),\n",
    "    ])\n",
    "    im_temp = preprocess(im_temp)\n",
    "    images.append(im_temp)\n",
    "\n",
    "images = torch.stack(images).to('cuda:0')\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(images).cpu().numpy()\n",
    "\n",
    "predicted_valid_wids = []\n",
    "for i in range(len(im_valid_test)):\n",
    "    predicted_valid_wids.append(pprint_output(out[i],1000))\n",
    "predicted_valid_wids = np.asarray(predicted_valid_wids)\n",
    "\n",
    "# Count errors and save baseline ranks\n",
    "count, error  = top5accuracy(true_valid_wids, predicted_valid_wids)\n",
    "baseline_ranks  = np.asarray(get_ranks(true_valid_wids,predicted_valid_wids))\n",
    "\n",
    "print (baseline_ranks.shape)\n",
    "print('baseline '+str(count)+' '+str(len(true_valid_wids))+' '+str(error)+' '+str(1-error))\n",
    "\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 150528)\n",
      "(150528, 2)\n"
     ]
    }
   ],
   "source": [
    "classes = ['schooner','brain_coral','junco_bird','snail','grey_whale','siberian_husky','electric_fan','bookcase','fountain_pen','toaster']\n",
    "class_wids = ['n04147183','n01917289','n01534433','n01944390','n02066245','n02110185','n03271574','n02870880','n03388183','n04442312']\n",
    "\n",
    "class_idx = 0\n",
    "c = classes[class_idx]\n",
    "\n",
    "unit_activations = {l:[] for l in layers}\n",
    "for im in im_valid_test:\n",
    "    wid = im.split('/')[-1].split('.')[0].split('_')[-1]\n",
    "    with open(os.path.join(data_path,'activations',wid+'.pkl'), 'rb') as f:\n",
    "        activations = pickle.load(f)\n",
    "        for l in layers:\n",
    "            unit_activations[l].append(activations[l][:,1:,:].flatten())\n",
    "\n",
    "unit_activations = {l:np.row_stack(unit_activations[l]) for l in layers}\n",
    "print(unit_activations[layers[0]].shape)\n",
    "\n",
    "class_indexes = [idx for idx in range(len(true_valid_wids)) if true_valid_wids[idx]==class_wids[class_idx]]\n",
    "other_indexes = [idx for idx in range(len(true_valid_wids)) if true_valid_wids[idx]!=class_wids[class_idx]]\n",
    "class_activations = {l:np.mean(unit_activations[l][class_indexes],axis=0) for l in layers}\n",
    "other_activations = {l:np.mean(unit_activations[l][other_indexes],axis=0) for l in layers}\n",
    "\n",
    "X = {l:np.column_stack((class_activations[l],other_activations[l])) for l in layers}\n",
    "\n",
    "print(X[layers[0]].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating grid\n",
      "37633 5 150526\n",
      "37632 25 150522\n",
      "37633 0 150527\n",
      "37631 2 150518\n"
     ]
    }
   ],
   "source": [
    "layer = 'encoder.layers.encoder_layer_10'\n",
    "\n",
    "magnitude = activation_to_magnitude(X[layer])\n",
    "selectivity = activation_to_selectivity(X[layer])\n",
    "\n",
    "print ('generating grid')\n",
    "x_partitions,y_partitions = 2,2\n",
    "cell = grid_space(magnitude,selectivity,x_partitions=x_partitions,y_partitions=y_partitions)\n",
    "\n",
    "# Don't have access to geopandas in python 2.7, so we're brute forcing unit assignments to cells.\n",
    "units_in_cells = dict()\n",
    "for cell_index in range(len(cell)):\n",
    "    units_in_cells[cell_index] = []\n",
    "    for unit_index in range(len(magnitude)):\n",
    "        if (magnitude[unit_index] >= cell[cell_index][0] and\n",
    "            selectivity[unit_index] >= cell[cell_index][1] and\n",
    "            magnitude[unit_index] <= cell[cell_index][2] and\n",
    "            selectivity[unit_index] <= cell[cell_index][3]):\n",
    "            units_in_cells[cell_index].append(unit_index)\n",
    "    print (len(units_in_cells[cell_index]),\n",
    "            min(units_in_cells[cell_index]),\n",
    "            max(units_in_cells[cell_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell:  0  Units:  37633\n",
      "363.1 847.9155555555556\n",
      "schooner 16 50 0.6799999999999999 0.32000000000000006\n",
      "Cell:  1  Units:  37632\n",
      "363.1 847.9155555555556\n",
      "schooner 16 50 0.6799999999999999 0.32000000000000006\n",
      "Cell:  2  Units:  37633\n",
      "363.1 847.9155555555556\n",
      "schooner 16 50 0.6799999999999999 0.32000000000000006\n",
      "Cell:  3  Units:  37631\n",
      "363.1 847.9155555555556\n",
      "schooner 16 50 0.6799999999999999 0.32000000000000006\n"
     ]
    }
   ],
   "source": [
    "for bbx in range(len(cell)):\n",
    "    # Query indices of units in that cell, create mask and set activations to zero\n",
    "    loc_new = units_in_cells[bbx]\n",
    "    # loc_new = np.random.choice(np.arange(768),size=750,replace=False)\n",
    "    lambda_mask = np.ones(shape=((len(magnitude),)),dtype=np.float32)\n",
    "\n",
    "    lambda_mask[loc_new] = 1.\n",
    "    lambda_mask = np.row_stack((np.zeros(768,dtype=np.float32),lambda_mask.reshape(196,768)))\n",
    "    print('Cell: ', bbx, ' Units: ', len(loc_new))\n",
    "\n",
    "    model = torchvision.models.vit_b_16(weights='DEFAULT')\n",
    "    hooked_model = HookedModel(model).to('cuda:0')\n",
    "    mask_hook = OutputMaskHook(torch.from_numpy(lambda_mask).to('cuda:0'))\n",
    "    hooked_model.apply_hook(layer,mask_hook)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = hooked_model(images).cpu().numpy()\n",
    "\n",
    "    predicted_valid_wids = []\n",
    "    for i in range(len(im_valid_test)):\n",
    "        predicted_valid_wids.append(pprint_output(out[i],1000))\n",
    "    predicted_valid_wids = np.asarray(predicted_valid_wids)\n",
    "\n",
    "    # calculate ranks\n",
    "    count, error  = top5accuracy(true_valid_wids[class_indexes], predicted_valid_wids[class_indexes])\n",
    "    class_ranks  = get_ranks(true_valid_wids[class_indexes],\n",
    "                            predicted_valid_wids[class_indexes])\n",
    "    other_ranks  = get_ranks(true_valid_wids[other_indexes],\n",
    "                            predicted_valid_wids[other_indexes])\n",
    "    class_mrd = mean_rank_deficit(baseline_ranks[class_indexes],class_ranks)\n",
    "    other_mrd = mean_rank_deficit(baseline_ranks[other_indexes],other_ranks)\n",
    "\n",
    "    print(class_mrd,other_mrd)\n",
    "    print(c+' '+str(count)+' '+str(len(class_indexes))+' '+str(error)+' '+str(1-error))\n",
    "\n",
    "    del hooked_model\n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('../data/','activations','00000011.pkl'), 'rb') as f:\n",
    "    activations = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 197, 768)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations['encoder.layers.encoder_layer_0'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rls(X,Y,penalty=0):\n",
    "    return (torch.linalg.inv(\n",
    "                X.T @ X + penalty * X.shape[0] * torch.eye(X.shape[1],dtype=X.dtype,device=X.device)) \n",
    "            @ X.T @ Y)\n",
    "\n",
    "def acc(X,Y,W):\n",
    "    predictions = torch.argmax(X @ W, 1)\n",
    "    labels = torch.argmax(Y, 1)\n",
    "    return torch.count_nonzero(predictions==labels)/len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 197, 768)\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.vit_b_16(weights='DEFAULT')\n",
    "layers = [n for n,m in model.named_modules() if isinstance(m,torchvision.models.vision_transformer.EncoderBlock)]\n",
    "del model\n",
    "\n",
    "# Get list of all images\n",
    "im_valid_test = glob.glob('../data/images/*')\n",
    "im_valid_test = np.asarray(im_valid_test)\n",
    "\n",
    "unit_activations = {l:[] for l in layers}\n",
    "for im in im_valid_test:\n",
    "    wid = im.split('/')[-1].split('.')[0].split('_')[-1]\n",
    "    with open(os.path.join('../data/','activations',wid+'.pkl'), 'rb') as f:\n",
    "        activations = pickle.load(f)\n",
    "        for l in layers:\n",
    "            unit_activations[l].append(activations[l])\n",
    "\n",
    "unit_activations = {l:np.row_stack(unit_activations[l]) for l in layers}\n",
    "print(unit_activations[layers[0]].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "wid_to_label = dict(zip(np.unique(true_valid_wids),np.arange(10)))\n",
    "labels = np.asarray([np.zeros(10)-1 for w in true_valid_wids])\n",
    "for i,w in enumerate(true_valid_wids):\n",
    "    labels[i,wid_to_label[w]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "for layer in range(12):\n",
    "    X = torch.from_numpy(unit_activations[layers[layer]][:400,0,:]).squeeze()\n",
    "    Y = torch.from_numpy(labels[:400]).float()\n",
    "    W = rls(X,Y,100)\n",
    "\n",
    "    X_test = torch.from_numpy(unit_activations[layers[layer]][400:,0,:]).squeeze()\n",
    "    Y_test = torch.from_numpy(labels[400:]).float()\n",
    "    accs.append(acc(X_test,Y_test,W).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(0.07, dtype=float32),\n",
       " array(0.07, dtype=float32),\n",
       " array(0.1, dtype=float32),\n",
       " array(0.12, dtype=float32),\n",
       " array(0.12, dtype=float32),\n",
       " array(0.15, dtype=float32),\n",
       " array(0.27, dtype=float32),\n",
       " array(0.53, dtype=float32),\n",
       " array(0.96, dtype=float32),\n",
       " array(0.99, dtype=float32),\n",
       " array(1., dtype=float32),\n",
       " array(0.99, dtype=float32)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/snag-lab/Documents/rsa_tmlr_2023/code/visualizations.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B128.138.225.154/home/snag-lab/Documents/rsa_tmlr_2023/code/visualizations.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m X \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfrom_numpy(np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mrandn(\u001b[39m500\u001b[39;49m,\u001b[39m15052\u001b[39;49m))\u001b[39m.\u001b[39;49mfloat()\u001b[39m.\u001b[39;49mto(\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B128.138.225.154/home/snag-lab/Documents/rsa_tmlr_2023/code/visualizations.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m Y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m,(\u001b[39m500\u001b[39m,\u001b[39m10\u001b[39m)))\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch113/lib/python3.10/site-packages/torch/cuda/__init__.py:247\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39menviron:\n\u001b[1;32m    246\u001b[0m     os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mLAZY\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 247\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[1;32m    248\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    251\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW"
     ]
    }
   ],
   "source": [
    "X = torch.from_numpy(np.random.randn(500,15052)).float().to('cuda')\n",
    "Y = torch.from_numpy(np.random.randint(0,2,(500,10))).float().to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.5 ms ± 1.95 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "w = rls(X,Y,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = rls(X,Y,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., -1.],\n",
       "       [-1.,  1.],\n",
       "       [-1.,  1.],\n",
       "       [-1.,  1.],\n",
       "       [-1.,  1.],\n",
       "       [-1.,  1.],\n",
       "       [-1.,  1.],\n",
       "       [ 1., -1.],\n",
       "       [-1.,  1.],\n",
       "       [ 1., -1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    return (2*np.eye(num_classes)-1)[y]\n",
    "\n",
    "to_categorical(np.random.randint(0,2,10),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fa96e51cbcdbc8dac59caac5cc17b1c244c32cd170c2be4d5299f9e41ace7a3c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.9 ('torch113')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
